# src/dataset.py
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import os
import traceback

class NoisePromptSequenceDataset(Dataset):
    """
    PyTorch Dataset for loading Noise Prompt Dataset (NPD) sequences.
    Assumes dataset was generated by the sequence version of generate_npd.py
    with a metadata.csv and corresponding .pt files for source noise
    and the golden noise sequence (a list of tensors).
    """
    def __init__(self, metadata_file, dataset_dir, max_seq_len=None):
        """
        Args:
            metadata_file (str): Path to the metadata.csv file.
            dataset_dir (str): Path to the root directory of the dataset.
            max_seq_len (int, optional): If provided, truncate loaded sequences
                                        to this maximum length. Defaults to None (use full sequence).
        """
        super().__init__()
        self.dataset_dir = dataset_dir
        self.max_seq_len = max_seq_len
        try:
            self.metadata = pd.read_csv(metadata_file)
            print(f"Loaded sequence metadata with {len(self.metadata)} entries.")
            # Ensure required columns exist
            required_cols = ['prompt', 'source_noise_file', 'golden_sequence_file', 'num_steps']
            if not all(col in self.metadata.columns for col in required_cols):
                    raise ValueError(f"Metadata file missing one or more required columns: {required_cols}")

        except FileNotFoundError:
            print(f"Error: Metadata file not found at {metadata_file}")
            self.metadata = pd.DataFrame()
        except Exception as e:
            print(f"Error reading metadata file {metadata_file}: {e}")
            self.metadata = pd.DataFrame()

        # Prepend dataset_dir to file paths if they are relative
        if not self.metadata.empty:
                # Check if paths are already absolute
                is_abs_source = os.path.isabs(self.metadata['source_noise_file'].iloc[0])
                is_abs_seq = os.path.isabs(self.metadata['golden_sequence_file'].iloc[0])

                if not is_abs_source:
                    self.metadata['source_noise_file'] = self.metadata['source_noise_file'].apply(lambda x: os.path.join(self.dataset_dir, x))
                if not is_abs_seq:
                    self.metadata['golden_sequence_file'] = self.metadata['golden_sequence_file'].apply(lambda x: os.path.join(self.dataset_dir, x))


    def __len__(self):
        """ Returns the number of samples in the dataset. """
        return len(self.metadata)

    def __getitem__(self, idx):
        """
        Loads and returns a sample from the dataset at the given index.

        Returns:
            dict: A dictionary containing:
                'prompt': The text prompt (str).
                'source_noise': The source noise tensor [C, H, W] (torch.Tensor).
                'golden_sequence': The sequence of golden noise tensors [SeqLen, C, H, W] (torch.Tensor).
                'seq_len': The actual length of the golden sequence (int).
        """
        if idx >= len(self.metadata):
            raise IndexError("Index out of bounds")

        data_row = self.metadata.iloc[idx]

        prompt = data_row.get('prompt', '')
        source_noise_path = data_row.get('source_noise_file')
        sequence_path = data_row.get('golden_sequence_file')
        expected_seq_len = data_row.get('num_steps', 0)

        if not source_noise_path or not sequence_path:
                print(f"Warning: Missing file path for index {idx}. Skipping.")
                return None

        # Load tensors
        try:
            source_noise = torch.load(source_noise_path, map_location='cpu').to(torch.float32)
            # Load the list of tensors for the sequence
            golden_sequence_list = torch.load(sequence_path, map_location='cpu')

            # Validate sequence
            if not isinstance(golden_sequence_list, list) or not golden_sequence_list:
                    print(f"Warning: Golden sequence file {sequence_path} for index {idx} is not a valid list or is empty. Skipping.")
                    return None

            # Stack the list into a single tensor [SeqLen, C, H, W] and convert dtype
            golden_sequence = torch.stack([t.to(torch.float32) for t in golden_sequence_list], dim=0)

            # Verify sequence length
            actual_seq_len = golden_sequence.shape[0]
            if actual_seq_len != expected_seq_len:
                    print(f"Warning: Sequence length mismatch for index {idx}. Metadata: {expected_seq_len}, Loaded: {actual_seq_len}. Using loaded length.")
                    # Decide how to handle: skip, truncate, use actual? Let's use actual for now.

            # Truncate sequence if max_seq_len is set
            if self.max_seq_len is not None and actual_seq_len > self.max_seq_len:
                golden_sequence = golden_sequence[:self.max_seq_len]
                actual_seq_len = self.max_seq_len
                # print(f"Debug: Truncated sequence for index {idx} to {actual_seq_len}")


        except FileNotFoundError as e:
            print(f"Error loading file: {e}. Skipping index {idx}.")
            return None
        except Exception as e:
                print(f"Error loading or processing data for index {idx}: {e}")
                traceback.print_exc()
                return None

        # Ensure source_noise has 4 dims [1, C, H, W] if needed later, but dataset usually returns single item
        # Let's return [C, H, W] for source noise and [SeqLen, C, H, W] for sequence
        if source_noise.dim() == 4 and source_noise.shape[0] == 1:
                source_noise = source_noise.squeeze(0) # Remove batch dim if present

        return {
            'prompt': prompt,
            'source_noise': source_noise,     # Shape [C, H, W]
            'golden_sequence': golden_sequence, # Shape [SeqLen, C, H, W]
            'seq_len': actual_seq_len         # Actual length of the sequence tensor
        }

def sequence_collate_fn(batch):
    """
    Custom collate function for sequence data. Handles None values and batches tensors.
    Handles variable sequence lengths by padding (if necessary, though less common for noise).
    """
    batch = [item for item in batch if item is not None]
    if not batch:
        return None

    prompts = [item['prompt'] for item in batch]
    source_noises = torch.stack([item['source_noise'] for item in batch]) # [B, C, H, W]
    seq_lens = [item['seq_len'] for item in batch]

    # --- Batching Sequences ---
    # Since all sequences in this dataset generation should have the same length (num_steps),
    # padding might not be strictly necessary. We can just stack them.
    # If lengths could vary (e.g., due to truncation or errors), padding would be needed.
    try:
            golden_sequences = torch.stack([item['golden_sequence'] for item in batch]) # [B, SeqLen, C, H, W]
    except RuntimeError as e:
            # This might happen if sequences have different lengths after all
            print(f"Error stacking sequences (likely due to varying lengths): {e}")
            # Implement padding if needed:
            # max_len = max(seq_lens)
            # padded_sequences = []
            # for item in batch:
            #     seq = item['golden_sequence']
            #     pad_len = max_len - seq.shape[0]
            #     if pad_len > 0:
            #         padding = torch.zeros((pad_len, *seq.shape[1:]), dtype=seq.dtype)
            #         seq = torch.cat([seq, padding], dim=0)
            #     padded_sequences.append(seq)
            # golden_sequences = torch.stack(padded_sequences)
            # print(f"Warning: Padded sequences to max length {max_len}")
            return None # Or handle padding


    return {
        'prompt': prompts,
        'source_noise': source_noises,
        'golden_sequence': golden_sequences,
        'seq_len': torch.tensor(seq_lens) # Keep track of original lengths if padding was used
    }


def get_sequence_dataloader(metadata_file, dataset_dir, batch_size, max_seq_len=None, shuffle=True, num_workers=4):
    """ Creates a DataLoader for the NoisePromptSequenceDataset. """
    dataset = NoisePromptSequenceDataset(
        metadata_file=metadata_file,
        dataset_dir=dataset_dir,
        max_seq_len=max_seq_len
    )
    if len(dataset) == 0:
            print("Warning: Sequence dataset is empty.")
            return None

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True,
        collate_fn=sequence_collate_fn, # Use the sequence collate function
        prefetch_factor=2 if num_workers > 0 else None
    )
    return dataloader

# Example Usage
if __name__ == '__main__':
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # Assume sequence dataset is in '../data/npd_sequence_dataset_sdxl'
    default_dataset_dir = os.path.abspath(os.path.join(script_dir, '..', 'data', 'npd_sequence_dataset_sdxl'))
    default_metadata = os.path.join(default_dataset_dir, 'metadata.csv')

    print(f"Looking for sequence dataset in: {default_dataset_dir}")
    print(f"Looking for metadata file: {default_metadata}")

    if os.path.exists(default_metadata):
        print("Attempting to load sample sequence batch...")
        # Example: Load sequences, max length 5
        dataloader = get_sequence_dataloader(
            metadata_file=default_metadata,
            dataset_dir=default_dataset_dir,
            batch_size=2,
            max_seq_len=5, # Example: truncate sequences longer than 5
            shuffle=False,
            num_workers=0
        )

        if dataloader:
            try:
                sample_batch = next(iter(dataloader))
                if sample_batch:
                    print("\nSample Sequence Batch Loaded Successfully:")
                    print("Prompts:", sample_batch['prompt'])
                    print("Source Noise Shape:", sample_batch['source_noise'].shape)
                    print("Golden Sequence Shape:", sample_batch['golden_sequence'].shape) # [B, SeqLen, C, H, W]
                    print("Sequence Lengths:", sample_batch['seq_len'])
                    print("Golden Sequence dtype:", sample_batch['golden_sequence'].dtype)
                else:
                        print("Failed to load a valid batch.")
            except StopIteration:
                print("DataLoader is empty or could not iterate.")
            except Exception as e:
                print(f"Error getting sample batch: {e}")
                traceback.print_exc()
        else:
            print("Failed to create DataLoader.")
    else:
        print("\nDefault metadata file not found. Cannot test DataLoader.")
        print("Please generate the sequence dataset first.")

