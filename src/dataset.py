# src/dataset.py
import torch
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import os

class NoisePromptDataset(Dataset):
    """
    PyTorch Dataset for loading Noise Prompt Dataset (NPD) pairs.
    Assumes dataset was generated by generate_npd.py with a metadata.csv
    and corresponding .pt files for source and target noise.
    """
    def __init__(self, metadata_file, dataset_dir):
        """
        Args:
            metadata_file (str): Path to the metadata.csv file.
            dataset_dir (str): Path to the root directory of the dataset
                                (where metadata.csv resides). Used to resolve
                                relative paths in the metadata file.
        """
        super().__init__()
        self.dataset_dir = dataset_dir
        try:
            self.metadata = pd.read_csv(metadata_file)
            print(f"Loaded metadata with {len(self.metadata)} entries.")
        except FileNotFoundError:
            print(f"Error: Metadata file not found at {metadata_file}")
            self.metadata = pd.DataFrame() # Empty dataframe
        except Exception as e:
            print(f"Error reading metadata file {metadata_file}: {e}")
            self.metadata = pd.DataFrame()

        # Prepend dataset_dir to file paths if they are relative
        if not self.metadata.empty:
                if 'source_noise_file' in self.metadata.columns and not os.path.isabs(self.metadata['source_noise_file'][0]):
                    self.metadata['source_noise_file'] = self.metadata['source_noise_file'].apply(lambda x: os.path.join(self.dataset_dir, x))
                if 'target_noise_file' in self.metadata.columns and not os.path.isabs(self.metadata['target_noise_file'][0]):
                    self.metadata['target_noise_file'] = self.metadata['target_noise_file'].apply(lambda x: os.path.join(self.dataset_dir, x))


    def __len__(self):
        """ Returns the number of samples in the dataset. """
        return len(self.metadata)

    def __getitem__(self, idx):
        """
        Loads and returns a sample from the dataset at the given index.

        Returns:
            dict: A dictionary containing:
                'prompt': The text prompt (str).
                'source_noise': The source noise tensor (torch.Tensor).
                'target_noise': The target (golden) noise tensor (torch.Tensor).
        """
        if idx >= len(self.metadata):
            raise IndexError("Index out of bounds")

        data_row = self.metadata.iloc[idx]

        prompt = data_row.get('prompt', '') # Get prompt, default to empty string if missing
        source_noise_path = data_row.get('source_noise_file')
        target_noise_path = data_row.get('target_noise_file')

        # Load noise tensors
        try:
            # Load directly to CPU first to avoid OOM on GPU during loading
            source_noise = torch.load(source_noise_path, map_location='cpu')
            target_noise = torch.load(target_noise_path, map_location='cpu')

            # Ensure they are float32, as NPNet expects float32 internally
            source_noise = source_noise.to(torch.float32)
            target_noise = target_noise.to(torch.float32)

        except FileNotFoundError as e:
            print(f"Error loading noise file: {e}. Skipping index {idx}.")
            # Return None or raise error, or return dummy data
            # Returning dummy data might hide issues during training
            # Let's return None and handle it in the collate_fn or dataloader loop
            return None
        except Exception as e:
                print(f"Error loading or processing data for index {idx}: {e}")
                return None


        # We need the prompt embeddings for NPNet input.
        # Generating them here is inefficient. They should ideally be pre-computed
        # and stored, or generated by the training script's main model pipeline.
        # For now, we only return the prompt text. The training script will handle embedding.
        return {
            'prompt': prompt,
            'source_noise': source_noise,
            'target_noise': target_noise
        }

def collate_fn(batch):
    """
    Custom collate function to handle potential None values from __getitem__.
    Also batches the tensors correctly.
    """
    # Filter out None items
    batch = [item for item in batch if item is not None]
    if not batch:
        return None # Return None if the whole batch was invalid

    # Collate different fields
    prompts = [item['prompt'] for item in batch]
    source_noises = torch.stack([item['source_noise'] for item in batch])
    target_noises = torch.stack([item['target_noise'] for item in batch])

    return {
        'prompt': prompts, # List of strings
        'source_noise': source_noises, # Batched tensor
        'target_noise': target_noises # Batched tensor
    }


def get_dataloader(metadata_file, dataset_dir, batch_size, shuffle=True, num_workers=4):
    """
    Creates a DataLoader for the NoisePromptDataset.

    Args:
        metadata_file (str): Path to the metadata.csv file.
        dataset_dir (str): Path to the root directory of the dataset.
        batch_size (int): Number of samples per batch.
        shuffle (bool): Whether to shuffle the data at every epoch. Defaults to True.
        num_workers (int): How many subprocesses to use for data loading. Defaults to 4.

    Returns:
        DataLoader: The PyTorch DataLoader instance.
    """
    dataset = NoisePromptDataset(metadata_file=metadata_file, dataset_dir=dataset_dir)
    if len(dataset) == 0:
            print("Warning: Dataset is empty. DataLoader cannot be created.")
            return None

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=shuffle,
        num_workers=num_workers,
        pin_memory=True, # Helps speed up data transfer to GPU
        collate_fn=collate_fn, # Use custom collate function
        prefetch_factor=2 if num_workers > 0 else None
    )
    return dataloader

# Example Usage (if run directly)
if __name__ == '__main__':
    # Assume dataset generated in '../data/npd_dataset_sdxl' relative to this script
    script_dir = os.path.dirname(os.path.abspath(__file__))
    default_dataset_dir = os.path.abspath(os.path.join(script_dir, '..', 'data', 'npd_dataset_sdxl'))
    default_metadata = os.path.join(default_dataset_dir, 'metadata.csv')

    print(f"Looking for dataset in: {default_dataset_dir}")
    print(f"Looking for metadata file: {default_metadata}")


    if os.path.exists(default_metadata):
        print("Attempting to load sample batch...")
        dataloader = get_dataloader(
            metadata_file=default_metadata,
            dataset_dir=default_dataset_dir,
            batch_size=4,
            shuffle=False,
            num_workers=0 # Use 0 for simple testing
        )

        if dataloader:
            try:
                sample_batch = next(iter(dataloader))
                if sample_batch:
                    print("\nSample Batch Loaded Successfully:")
                    print("Prompts:", sample_batch['prompt'])
                    print("Source Noise Shape:", sample_batch['source_noise'].shape)
                    print("Target Noise Shape:", sample_batch['target_noise'].shape)
                    print("Source Noise dtype:", sample_batch['source_noise'].dtype)
                    print("Target Noise dtype:", sample_batch['target_noise'].dtype)
                else:
                        print("Failed to load a valid batch (collate_fn returned None).")
            except StopIteration:
                print("DataLoader is empty or could not iterate.")
            except Exception as e:
                print(f"Error getting sample batch: {e}")
        else:
            print("Failed to create DataLoader.")
    else:
        print("\nDefault metadata file not found. Cannot test DataLoader.")
        print("Please generate the dataset using generate_npd.py first,")
        print("or ensure the path is correct.")
